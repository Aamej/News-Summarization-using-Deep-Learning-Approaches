{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc8e75cc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-09T18:51:51.173715Z",
     "iopub.status.busy": "2022-11-09T18:51:51.173359Z",
     "iopub.status.idle": "2022-11-09T18:51:51.197257Z",
     "shell.execute_reply": "2022-11-09T18:51:51.196283Z"
    },
    "papermill": {
     "duration": 0.032034,
     "end_time": "2022-11-09T18:51:51.199951",
     "exception": false,
     "start_time": "2022-11-09T18:51:51.167917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv\n",
      "/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv\n",
      "/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135c450f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-09T18:51:51.209522Z",
     "iopub.status.busy": "2022-11-09T18:51:51.209225Z",
     "iopub.status.idle": "2022-11-09T18:52:35.403473Z",
     "shell.execute_reply": "2022-11-09T18:52:35.402143Z"
    },
    "papermill": {
     "duration": 44.201532,
     "end_time": "2022-11-09T18:52:35.405944",
     "exception": false,
     "start_time": "2022-11-09T18:51:51.204412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-extractive-summarizer\r\n",
      "  Downloading bert_extractive_summarizer-0.10.1-py3-none-any.whl (25 kB)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from bert-extractive-summarizer) (4.20.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from bert-extractive-summarizer) (1.0.2)\r\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (from bert-extractive-summarizer) (3.3.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->bert-extractive-summarizer) (3.1.0)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->bert-extractive-summarizer) (1.7.3)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->bert-extractive-summarizer) (1.0.1)\r\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->bert-extractive-summarizer) (1.21.6)\r\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (0.4.2)\r\n",
      "Collecting typing-extensions<4.2.0,>=3.7.4\r\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (2.0.8)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (3.1.2)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (3.0.10)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (2.28.1)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (1.8.2)\r\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (8.0.17)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (59.8.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (4.64.0)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (2.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (21.3)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (3.0.8)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (0.10.1)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (0.7.9)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (2.0.7)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (0.6.2)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (1.0.3)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (3.3.0)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy->bert-extractive-summarizer) (1.0.9)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (2021.11.10)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (3.7.1)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (0.12.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (4.13.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers->bert-extractive-summarizer) (0.10.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy->bert-extractive-summarizer) (3.8.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy->bert-extractive-summarizer) (3.0.9)\r\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy->bert-extractive-summarizer) (5.2.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2022.9.24)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.26.12)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.1.0)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy->bert-extractive-summarizer) (8.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy->bert-extractive-summarizer) (2.1.1)\r\n",
      "Installing collected packages: typing-extensions, bert-extractive-summarizer\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.4.0\r\n",
      "    Uninstalling typing_extensions-4.4.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.4.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\r\n",
      "tensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\r\n",
      "tensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\r\n",
      "tensorflow 2.6.4 requires tensorboard<2.7,>=2.6.0, but you have tensorboard 2.10.1 which is incompatible.\r\n",
      "tensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.1.1 which is incompatible.\r\n",
      "tensorflow-transform 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\r\n",
      "tensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\r\n",
      "tensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\r\n",
      "pandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\r\n",
      "flake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\r\n",
      "apache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\r\n",
      "apache-beam 2.40.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 8.0.0 which is incompatible.\r\n",
      "allennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\r\n",
      "aiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.93 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed bert-extractive-summarizer-0.10.1 typing-extensions-4.1.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting rouge\r\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rouge) (1.15.0)\r\n",
      "Installing collected packages: rouge\r\n",
      "Successfully installed rouge-1.0.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-extractive-summarizer\n",
    "!pip install rouge\n",
    "from rouge import Rouge\n",
    "import sys\n",
    "import string\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from summarizer import Summarizer, TransformerSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ee0bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-09T18:52:35.418354Z",
     "iopub.status.busy": "2022-11-09T18:52:35.416019Z",
     "iopub.status.idle": "2022-11-09T18:52:37.726111Z",
     "shell.execute_reply": "2022-11-09T18:52:37.725142Z"
    },
    "papermill": {
     "duration": 2.317917,
     "end_time": "2022-11-09T18:52:37.728514",
     "exception": false,
     "start_time": "2022-11-09T18:52:35.410597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('../input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv')\n",
    "test_df = pd.read_csv('../input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb086b10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-09T18:52:37.739793Z",
     "iopub.status.busy": "2022-11-09T18:52:37.739225Z",
     "iopub.status.idle": "2022-11-09T18:52:38.038095Z",
     "shell.execute_reply": "2022-11-09T18:52:38.037287Z"
    },
    "papermill": {
     "duration": 0.306578,
     "end_time": "2022-11-09T18:52:38.040078",
     "exception": false,
     "start_time": "2022-11-09T18:52:37.733500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c0b1be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-09T18:52:38.050339Z",
     "iopub.status.busy": "2022-11-09T18:52:38.050035Z",
     "iopub.status.idle": "2022-11-09T18:52:38.342011Z",
     "shell.execute_reply": "2022-11-09T18:52:38.340928Z"
    },
    "papermill": {
     "duration": 0.299646,
     "end_time": "2022-11-09T18:52:38.344234",
     "exception": false,
     "start_time": "2022-11-09T18:52:38.044588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e8f9d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-09T18:52:38.355046Z",
     "iopub.status.busy": "2022-11-09T18:52:38.354713Z",
     "iopub.status.idle": "2022-11-09T18:52:38.680283Z",
     "shell.execute_reply": "2022-11-09T18:52:38.678740Z"
    },
    "papermill": {
     "duration": 0.333383,
     "end_time": "2022-11-09T18:52:38.682359",
     "exception": false,
     "start_time": "2022-11-09T18:52:38.348976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(13368, 3)\n"
     ]
    }
   ],
   "source": [
    "print(val_df.duplicated(subset=['article', 'highlights']).sum())\n",
    "val_df = val_df.drop_duplicates(subset=['article','highlights'])\n",
    "val_df.reset_index()\n",
    "print(val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf520d42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-09T18:52:38.693590Z",
     "iopub.status.busy": "2022-11-09T18:52:38.692648Z",
     "iopub.status.idle": "2022-11-09T18:52:38.967825Z",
     "shell.execute_reply": "2022-11-09T18:52:38.966545Z"
    },
    "papermill": {
     "duration": 0.282538,
     "end_time": "2022-11-09T18:52:38.969627",
     "exception": false,
     "start_time": "2022-11-09T18:52:38.687089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(11488, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_df.duplicated(subset=['article', 'highlights']).sum())\n",
    "test_df = test_df.drop_duplicates(subset=['article','highlights'])\n",
    "test_df.reset_index()\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ebf5f7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-09T18:52:38.980469Z",
     "iopub.status.busy": "2022-11-09T18:52:38.980149Z",
     "iopub.status.idle": "2022-11-09T18:52:39.828359Z",
     "shell.execute_reply": "2022-11-09T18:52:39.827458Z"
    },
    "papermill": {
     "duration": 0.856002,
     "end_time": "2022-11-09T18:52:39.830436",
     "exception": false,
     "start_time": "2022-11-09T18:52:38.974434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_df['article_len'] = val_df.apply(lambda r: len(r['article'].split()), axis=1)\n",
    "val_df['highlights_len'] = val_df.apply(lambda r: len(r['highlights'].split()), axis=1)\n",
    "val_df = val_df.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47d49dfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-09T18:52:39.841624Z",
     "iopub.status.busy": "2022-11-09T18:52:39.841307Z",
     "iopub.status.idle": "2022-11-09T18:52:40.563960Z",
     "shell.execute_reply": "2022-11-09T18:52:40.562921Z"
    },
    "papermill": {
     "duration": 0.731283,
     "end_time": "2022-11-09T18:52:40.566541",
     "exception": false,
     "start_time": "2022-11-09T18:52:39.835258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df['article_len'] = test_df.apply(lambda r: len(r['article'].split()), axis=1)\n",
    "test_df['highlights_len'] = test_df.apply(lambda r: len(r['highlights'].split()), axis=1)\n",
    "test_df = test_df.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47a7f376",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-09T18:52:40.578958Z",
     "iopub.status.busy": "2022-11-09T18:52:40.578206Z",
     "iopub.status.idle": "2022-11-09T18:52:40.583765Z",
     "shell.execute_reply": "2022-11-09T18:52:40.582600Z"
    },
    "papermill": {
     "duration": 0.014017,
     "end_time": "2022-11-09T18:52:40.586111",
     "exception": false,
     "start_time": "2022-11-09T18:52:40.572094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "k_list = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf204070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-09T18:52:40.598284Z",
     "iopub.status.busy": "2022-11-09T18:52:40.597261Z",
     "iopub.status.idle": "2022-11-09T19:06:22.733681Z",
     "shell.execute_reply": "2022-11-09T19:06:22.732792Z"
    },
    "papermill": {
     "duration": 822.145515,
     "end_time": "2022-11-09T19:06:22.736794",
     "exception": false,
     "start_time": "2022-11-09T18:52:40.591279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13368/13368 [02:44<00:00, 81.25it/s]\n",
      "100%|██████████| 13368/13368 [02:43<00:00, 81.72it/s]\n",
      "100%|██████████| 13368/13368 [02:43<00:00, 81.55it/s]\n",
      "100%|██████████| 13368/13368 [02:43<00:00, 81.69it/s]\n",
      "100%|██████████| 13368/13368 [02:43<00:00, 81.65it/s]\n",
      "100%|██████████| 5/5 [13:42<00:00, 164.42s/it]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(k_list):\n",
    "    art_ext_list = []\n",
    "    for i in tqdm(val_df.index, position=0):\n",
    "        news = val_df.loc[i]\n",
    "        art = news['article']\n",
    "        high = news['highlights']\n",
    "        art = sent_tokenize(art)\n",
    "        orig_art = art\n",
    "\n",
    "        art_form = []\n",
    "        stop = stopwords.words('english')\n",
    "        punc = string.punctuation\n",
    "\n",
    "        for sent in art:\n",
    "            sent_form = []\n",
    "            words = word_tokenize(sent)\n",
    "            for word in words:\n",
    "                if word not in stop and word not in punc:\n",
    "                    sent_form.append(word.lower())\n",
    "            art_form.append(sent_form)\n",
    "\n",
    "        art_form_1 = []\n",
    "        art_form_len = []\n",
    "        lemma = WordNetLemmatizer()\n",
    "\n",
    "        for sent in art_form:\n",
    "            words = []\n",
    "            art_form_len.append(len(sent))\n",
    "            for word in sent:\n",
    "                lemma_word = lemma.lemmatize(word)\n",
    "                words.append(lemma_word)\n",
    "            art_form_1.append(words)\n",
    "\n",
    "        art_form = art_form_1\n",
    "\n",
    "        art_form_1 = []\n",
    "        for sen in art_form:\n",
    "            s = \" \".join(sen)\n",
    "            art_form_1.append(s)\n",
    "\n",
    "        art_form = art_form_1\n",
    "\n",
    "        vect = TfidfVectorizer()\n",
    "        tf_idf_mat = vect.fit_transform(art_form)\n",
    "        score = np.sum(tf_idf_mat, axis=1)\n",
    "        score = np.ravel(score)\n",
    "        n = max(int(len(art_form_len)/k),1)\n",
    "        idx = np.sort(np.argsort(np.array(score))[::-1][:n])\n",
    "        art_ext = [orig_art[ind] for ind in idx]\n",
    "        art_ext = \" \".join(art_ext)\n",
    "        art_ext_list.append(art_ext)\n",
    "\n",
    "    val_df_ext = pd.DataFrame()\n",
    "    val_df_ext[\"extracted_article\"] = art_ext_list\n",
    "    val_df_ext.set_index(val_df.index, inplace=True)\n",
    "    val_df_ext.to_csv('val/val_ext_'+str(k)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "854ca9fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-09T19:06:23.506997Z",
     "iopub.status.busy": "2022-11-09T19:06:23.506673Z",
     "iopub.status.idle": "2022-11-09T19:18:21.107259Z",
     "shell.execute_reply": "2022-11-09T19:18:21.106200Z"
    },
    "papermill": {
     "duration": 718.00248,
     "end_time": "2022-11-09T19:18:21.109082",
     "exception": false,
     "start_time": "2022-11-09T19:06:23.106602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11488/11488 [02:22<00:00, 80.61it/s]\n",
      "100%|██████████| 11488/11488 [02:23<00:00, 80.30it/s]\n",
      "100%|██████████| 11488/11488 [02:22<00:00, 80.49it/s]\n",
      "100%|██████████| 11488/11488 [02:23<00:00, 80.27it/s]\n",
      "100%|██████████| 11488/11488 [02:23<00:00, 79.90it/s]\n",
      "100%|██████████| 5/5 [11:57<00:00, 143.52s/it]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(k_list):\n",
    "    art_ext_list = []\n",
    "    for i in tqdm(test_df.index, position=0):\n",
    "        news = test_df.loc[i]\n",
    "        art = news['article']\n",
    "        high = news['highlights']\n",
    "        art = sent_tokenize(art)\n",
    "        orig_art = art\n",
    "\n",
    "        art_form = []\n",
    "        stop = stopwords.words('english')\n",
    "        punc = string.punctuation\n",
    "\n",
    "        for sent in art:\n",
    "            sent_form = []\n",
    "            words = word_tokenize(sent)\n",
    "            for word in words:\n",
    "                if word not in stop and word not in punc:\n",
    "                    sent_form.append(word.lower())\n",
    "            art_form.append(sent_form)\n",
    "\n",
    "        art_form_1 = []\n",
    "        art_form_len = []\n",
    "        lemma = WordNetLemmatizer()\n",
    "\n",
    "        for sent in art_form:\n",
    "            words = []\n",
    "            art_form_len.append(len(sent))\n",
    "            for word in sent:\n",
    "                lemma_word = lemma.lemmatize(word)\n",
    "                words.append(lemma_word)\n",
    "            art_form_1.append(words)\n",
    "\n",
    "        art_form = art_form_1\n",
    "\n",
    "        art_form_1 = []\n",
    "        for sen in art_form:\n",
    "            s = \" \".join(sen)\n",
    "            art_form_1.append(s)\n",
    "\n",
    "        art_form = art_form_1\n",
    "\n",
    "        vect = TfidfVectorizer()\n",
    "        tf_idf_mat = vect.fit_transform(art_form)\n",
    "        score = np.sum(tf_idf_mat, axis=1)\n",
    "        score = np.ravel(score)\n",
    "        n = max(int(len(art_form_len)/k),1)\n",
    "        idx = np.sort(np.argsort(np.array(score))[::-1][:n])\n",
    "        art_ext = [orig_art[ind] for ind in idx]\n",
    "        art_ext = \" \".join(art_ext)\n",
    "        art_ext_list.append(art_ext)\n",
    "\n",
    "    test_df_ext = pd.DataFrame()\n",
    "    test_df_ext[\"extracted_article\"] = art_ext_list\n",
    "    test_df_ext.set_index(test_df.index, inplace=True)\n",
    "    test_df_ext.to_csv('test/test_ext_'+str(k)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2c461",
   "metadata": {
    "papermill": {
     "duration": 0.742591,
     "end_time": "2022-11-09T19:18:22.532660",
     "exception": false,
     "start_time": "2022-11-09T19:18:21.790069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1601.773675,
   "end_time": "2022-11-09T19:18:26.155878",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-09T18:51:44.382203",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
